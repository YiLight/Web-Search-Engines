There is only one .py file in the directory. It contains all the code needed to run my crawler. 

Before running the code, you should check the requirements.txt file to get all the packets needed. 

When running the code, you should input 4 times to initial the crawler. 
1) you should input your key words, e.g. "brooklyn union". 
2) you should input your goal of successful urls to crawl, e.g. "10000".
3) you should input the number of threads you want, e.g. "50". 
4) you should input the mode of the crawler, "p" for priority mode and all other input for regular mode. 

After running the code, log of the crwaler will be appended to log.log file in the directory (new one will be created if needed). 

There are 4 logs, 1 readme.txt, 1 explain.txt, and 1 requirements.txt files in the directory, too. 